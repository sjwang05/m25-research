{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8741c0",
   "metadata": {},
   "source": [
    "# Convert Kyoto Hospital Medical Data to NutriBench Format w/ Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9d0c7",
   "metadata": {},
   "source": [
    "convert xls(x) to csv for easier viewing in a text editor (e.g., when validating the script's output)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3756c643",
   "metadata": {},
   "outputs": [],
   "source": "# xls2csv.py\nimport pandas as pd\nimport os\n\n# List all Excel files in the current directory\nexcel_files = [f for f in os.listdir('.') if f.endswith('.xlsx') or f.endswith('.xls')]\n\nfor file in excel_files:\n    try:\n        xl = pd.ExcelFile(file)\n        for sheet in xl.sheet_names:\n            df = xl.parse(sheet, header=None)\n            outname = f\"{os.path.splitext(file)[0]}_{sheet}.csv\"\n            df.to_csv(outname, index=False, header=False)\n            print(f\"Saved: {outname}\")\n    except Exception as e:\n        print(f\"Error processing {file}: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "54ae05c2",
   "metadata": {},
   "source": [
    "convert the hospital data to nutribench format (without EN translations for now)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0040b3a",
   "metadata": {},
   "outputs": [],
   "source": "# nhk2nutribench.py\nimport json\nimport re\nimport unicodedata\nfrom pathlib import Path\nfrom typing import Any, List, Dict, Tuple\n\nimport pandas as pd\n\n# Constants\nINPUT_CSV = Path(\"NHO-Kyoto.csv\")\nOUTPUT_CSV = Path(\"NHO-Kyoto-NutriBench.csv\")\nJSONL_OUTPUT = False\n\n# ---------------------------------------------------------------------------\n# Mapping and column indices\n# ---------------------------------------------------------------------------\nMEAL_MAP = {\"朝\": \"breakfast\", \"昼\": \"lunch\", \"夕\": \"dinner\"}\n\nCOL_TIME = 1       # \"朝\" / \"昼\" / \"夕\" or blank\nCOL_DISH = 3       # Dish name (料理名)\nCOL_SUBTOTAL = 7   # Label like \"朝小計\"\nCOL_ENERGY = 10    # kcal\nCOL_PROTEIN = 11   # g\nCOL_FAT = 14       # g\nCOL_CARB = 15      # g\n\n# ---------------------------------------------------------------------------\n# Text normalisation helpers\n# ---------------------------------------------------------------------------\n\ndef normalize_text(text: str) -> str:\n    \"\"\"Canonicalise *text* for matching & tidy output.\n\n    • half‑width kana → full‑width\n    • full‑width roman/ASCII variants → ASCII\n    • any \"space separator\" char → ASCII space\n    • collapse multiple whitespace → one space\n    \"\"\"\n    if not isinstance(text, str):\n        text = str(text or \"\")\n\n    s = unicodedata.normalize(\"NFKC\", text)  # width & punctuation normalisation\n    s = \"\".join(\" \" if unicodedata.category(c) == \"Zs\" else c for c in s)\n    s = re.sub(r\"\\s+\", \"\", s)\n    return s.strip()\n\n\n# ---------------------------------------------------------------------------\n# Numeric conversion helper\n# ---------------------------------------------------------------------------\n\ndef _to_float(cell: str | Any):\n    try:\n        return float(str(cell).replace(\",\", \"\")) if str(cell).strip() else None\n    except ValueError:\n        return None\n\n\n# ---------------------------------------------------------------------------\n# Core parsing routine\n# ---------------------------------------------------------------------------\n\ndef parse_kyoto(csv_path: Path) -> List[Dict[str, Any]]:\n    df = pd.read_csv(csv_path, header=None, dtype=str).fillna(\"\")\n\n    records: List[Dict[str, Any]] = []\n    current_meal_en: str | None = None\n    current_meal_ja: str | None = None\n    items: List[str] = []\n    energy_values: List[float | None] = []\n    protein_values: List[float | None] = []\n    fat_values: List[float | None] = []\n    carb_values: List[float | None] = []\n\n    for _, row in df.iterrows():\n        time_marker = normalize_text(row[COL_TIME])\n        dish_name = normalize_text(row[COL_DISH])\n        subtotal_label = normalize_text(row[COL_SUBTOTAL])\n\n        # --- start of a new meal ------------------------------------------\n        if current_meal_en is None:\n            if time_marker in MEAL_MAP and dish_name:\n                current_meal_en = MEAL_MAP[time_marker]\n                current_meal_ja = time_marker\n                items = [dish_name]\n                # Capture nutritional values for the first item\n                energy_values = [_to_float(row[COL_ENERGY])]\n                protein_values = [_to_float(row[COL_PROTEIN])]\n                fat_values = [_to_float(row[COL_FAT])]\n                carb_values = [_to_float(row[COL_CARB])]\n            continue\n\n        # --- inside a meal -------------------------------------------------\n        if (subtotal_label.endswith(\"小計\") and subtotal_label.startswith(current_meal_ja)):\n            # Check if we have any valid nutritional values\n            has_values = any(\n                v is not None for values in [energy_values, protein_values, fat_values, carb_values] \n                for v in values\n            )\n            \n            if has_values and items:  # Only add if we have both items and values\n                records.append({\n                    \"meal_time\": current_meal_en,\n                    \"meal_time_local\": current_meal_ja,\n                    \"description_local\": items.copy(),\n                    \"energy\": energy_values.copy(),\n                    \"protein\": protein_values.copy(),\n                    \"fat\": fat_values.copy(),\n                    \"carb\": carb_values.copy(),\n                })\n            \n            # Reset for next meal\n            current_meal_en = current_meal_ja = None\n            items = []\n            energy_values = []\n            protein_values = []\n            fat_values = []\n            carb_values = []\n            continue\n\n        # --- add food item and its nutritional values ----------------------\n        if dish_name and current_meal_en is not None:\n            # Get nutritional values for this item\n            energy = _to_float(row[COL_ENERGY])\n            protein = _to_float(row[COL_PROTEIN])\n            fat = _to_float(row[COL_FAT])\n            carb = _to_float(row[COL_CARB])\n            \n            # Only add if at least one nutritional value is present\n            if any(v is not None for v in [energy, protein, fat, carb]):\n                items.append(dish_name)\n                energy_values.append(energy)\n                protein_values.append(protein)\n                fat_values.append(fat)\n                carb_values.append(carb)\n\n    return records\n\n\n# ---------------------------------------------------------------------------\n# NutriBench DataFrame builder\n# ---------------------------------------------------------------------------\n\ndef build_nutribench_df(records: List[Dict[str, Any]]) -> pd.DataFrame:\n    # Convert arrays to JSON strings\n    for record in records:\n        record[\"description_local\"] = json.dumps(record[\"description_local\"], ensure_ascii=False)\n        record[\"carb\"] = json.dumps(record[\"carb\"], ensure_ascii=False)\n        record[\"energy\"] = json.dumps(record[\"energy\"], ensure_ascii=False)\n        record[\"protein\"] = json.dumps(record[\"protein\"], ensure_ascii=False)\n        record[\"fat\"] = json.dumps(record[\"fat\"], ensure_ascii=False)\n    \n    df = pd.DataFrame(records)\n\n    # Add empty arrays for fields not in the original data\n    df[\"description\"] = \"[]\"\n    df[\"unit\"] = \"[]\"\n    df[\"unit_local\"] = \"[]\"\n    df[\"local_language\"] = \"Japanese\"\n\n    order = [\n        \"description\", \"description_local\", \"unit\", \"unit_local\",\n        \"local_language\", \"carb\", \"energy\", \"protein\", \"fat\", \"meal_time\", \"meal_time_local\"\n    ]\n    return df[order]\n\n\n# ---------------------------------------------------------------------------\n# Main execution\n# ---------------------------------------------------------------------------\n\nrecords = parse_kyoto(INPUT_CSV)\nif not records:\n    raise SystemExit(\"No meals detected.\")\n\ndf = build_nutribench_df(records)\n\nif JSONL_OUTPUT:\n    df.to_json(OUTPUT_CSV.with_suffix('.jsonl'), orient=\"records\", lines=True, force_ascii=False)\nelse:\n    df.to_csv(OUTPUT_CSV, index=False)\n\nfmt = \"JSONL\" if JSONL_OUTPUT else \"CSV\"\nprint(f\"✅ Converted {len(df)} meals → {OUTPUT_CSV} ({fmt})\")"
  },
  {
   "cell_type": "markdown",
   "id": "62cf8bde",
   "metadata": {},
   "source": [
    "from the nutribench csv, extract a deduplicated list of foods to send to an LLM for translation"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1591f19",
   "metadata": {},
   "outputs": [],
   "source": "# ls_foods.py\nimport csv\nimport io\nimport re\nimport ast\n\n# Constants\nINPUT_FILENAME = \"NHO-Kyoto-NutriBench.csv\"\nOUTPUT_FILENAME = \"kyoto-foods.txt\"\n\ndef extract_unique_foods(csv_data):\n    \"\"\"\n    Parses CSV data to extract, clean, and deduplicate food items from\n    the 'description_local' column.\n\n    Args:\n        csv_data (str): A string containing the CSV data.\n\n    Returns:\n        list: A sorted list of unique food item names.\n    \"\"\"\n    # Use a set to store unique food items to automatically handle duplicates.\n    unique_foods = set()\n\n    # Use io.StringIO to treat the string data as a file for the csv reader\n    csv_file = io.StringIO(csv_data)\n    \n    # Use DictReader to easily access columns by name\n    reader = csv.DictReader(csv_file)\n\n    for row in reader:\n        # The 'description_local' column contains a string representation of a list.\n        # We use ast.literal_eval() to safely parse this string into a Python list.\n        try:\n            food_list_str = row['description_local']\n            # Safely evaluate the string to a list\n            food_items = ast.literal_eval(food_list_str)\n\n            if not isinstance(food_items, list):\n                continue\n                \n        except (ValueError, SyntaxError, KeyError):\n            # Skip rows where the column is missing or the format is incorrect\n            print(f\"Warning: Could not parse row: {row}\")\n            continue\n\n        for item in food_items:\n            if item:\n                unique_foods.add(item)\n\n    # Convert the set to a sorted list for a consistent output order.\n    return sorted(list(unique_foods))\n\n# --- Main execution block ---\n\ntry:\n    # Read the content from the specified CSV file.\n    with open(INPUT_FILENAME, 'r', encoding='utf-8') as f:\n        csv_content = f.read()\n    \n    # Process the file content to get the deduplicated list.\n    deduplicated_foods = extract_unique_foods(csv_content)\n\n    # Write the deduplicated list to the output file (one item per line)\n    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as out_f:\n        for food in deduplicated_foods:\n            out_f.write(f\"{food}\\n\")\n\n    # Print a summary to the console\n    print(f\"Deduplicated list of food items from '{INPUT_FILENAME}' written to '{OUTPUT_FILENAME}'.\")\n    print(f\"Total unique food items: {len(deduplicated_foods)}\")\n    if not deduplicated_foods:\n        print(\"No food items were found or extracted.\")\n\nexcept FileNotFoundError:\n    print(f\"Error: The file '{INPUT_FILENAME}' was not found. Please make sure it's in the same directory as the script.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "75f163f3",
   "metadata": {},
   "source": [
    "translate the list of food items from source language to english. instead of translating in a single shot, we chunk the file and ask the LLM to translate it 5 lines at a time.\n",
    "\n",
    "we do it this way because LLMs degrade as input context increases (https://fiction.live/stories/Fiction-liveBench-Mar-25-2025/oQdzQvKHw8JyXbN87 and other benchmarks), and as completion length increases (https://eqbench.com/creative_writing_longform.html, the \"degradation\" column). While these benchmarks focus on creative writing, I find it reasonable to extrapolate to tasks like translation as well\n",
    "\n",
    "we translate the entire deduplicated list instead of just iterating through each row of the nutribench csv because the nutribench csv might contain duplicate food items across rows - translating each food item only once saves on API costs."
   ]
  },
  {
   "cell_type": "code",
   "id": "d2003728",
   "metadata": {},
   "outputs": [],
   "source": "# translate_ls.py\nimport os\nfrom openai import OpenAI\n\n# Constants\nINPUT_FILE = \"kyoto-foods.txt\"\nOUTPUT_FILE = \"kyoto-foods-en.txt\"\nSRC_LANG = \"Japanese\"\nAPI_BASE_URL = \"https://openrouter.ai/api/v1\"\nAPI_KEY = os.environ.get(\"API_KEY\", os.environ.get(\"OPENROUTER_API_KEY\"))\nMODEL = \"google/gemini-2.5-pro\"\n\ndef process_chunk(client, model, chunk_lines):\n    \"\"\"Process a chunk of lines and return the translation.\"\"\"\n    # Join the lines into a single string\n    foods = \"\\n\".join(chunk_lines)\n    \n    # Create the prompt with the configured language\n    prompt = (\n        f\"You are an expert translator who specializes in nutrition and food items. \"\n        f\"Please translate the following list of foods from {SRC_LANG} to English. \"\n        f\"Only provide direct romanizations/transliterations when an average English speaker \"\n        f\"would be familiar with the romanization; otherwise, provide an English localization. \"\n        f\"Provide only the translated list.\\n\\n\"\n        f\"{foods}\"\n    )\n    \n    # Generate the response\n    completion = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        temperature=0.7, # slightly lower temp for (hopefully) more sane translations\n    )\n    \n    return completion.choices[0].message.content.strip()\n\n# Check if API key is available\nif not API_KEY:\n    print(\"Error: API_KEY or OPENROUTER_API_KEY environment variable is required\")\n    exit(1)\n\nclient = OpenAI(\n    base_url=API_BASE_URL,\n    api_key=API_KEY,\n)\n\nprint(f\"Using API: {API_BASE_URL}\")\nprint(f\"Using model: {MODEL}\")\n\ntry:\n    with open(INPUT_FILE, 'r', encoding='utf-8') as f_in, \\\n         open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n        \n        chunk = []\n        line_count = 0\n        total_chunks = 0\n        \n        for line in f_in:\n            line = line.strip()\n            if line:  # Skip empty lines\n                chunk.append(line)\n                line_count += 1\n            \n            # Process when we have 5 lines\n            if len(chunk) == 5:\n                total_chunks += 1\n                print(f\"Processing chunk {total_chunks} (lines {line_count-4} to {line_count})...\")\n                try:\n                    translation = process_chunk(client, MODEL, chunk)\n                    f_out.write(translation)\n                    if not translation.endswith('\\n'):\n                        f_out.write(\"\\n\")\n                    f_out.flush()  # Ensure output is written immediately\n                except Exception as e:\n                    print(f\"Error processing chunk {total_chunks}: {e}\")\n                    # Write original lines as fallback\n                    f_out.write(\"# Translation failed:\\n\")\n                    f_out.write(\"\\n\".join(chunk))\n                    f_out.write(\"\\n\")\n                chunk = []\n        \n        # Process any remaining lines\n        if chunk:\n            total_chunks += 1\n            print(f\"Processing final chunk {total_chunks} ({len(chunk)} lines)...\")\n            try:\n                translation = process_chunk(client, MODEL, chunk)\n                f_out.write(translation)\n                if not translation.endswith('\\n'):\n                    f_out.write(\"\\n\")\n            except Exception as e:\n                print(f\"Error processing final chunk: {e}\")\n                # Write original lines as fallback\n                f_out.write(\"# Translation failed:\\n\")\n                f_out.write(\"\\n\".join(chunk))\n    \n    print(f\"Translation complete! Processed {total_chunks} chunks.\")\n    print(f\"Output saved to {OUTPUT_FILE}\")\n    \nexcept FileNotFoundError:\n    print(f\"Error: Could not find input file '{INPUT_FILE}'\")\nexcept Exception as e:\n    print(f\"Error: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "4320c110",
   "metadata": {},
   "source": [
    "zip the source language and target language lists into a single csv translation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "id": "0d0c8e0b",
   "metadata": {},
   "outputs": [],
   "source": "# mk_dict.py\nimport csv\n\n# Constants\nJP_FILE = \"kyoto-foods.txt\"\nEN_FILE = \"kyoto-foods-en.txt\"\nOUTPUT_FILE = \"kyoto-foods-dict.csv\"\n\ndef create_food_csv(jp_file, en_file, output_file):\n    \"\"\"\n    Reads Japanese and English food names from separate files and combines them into a CSV.\n    \n    Args:\n        jp_file: Path to the Japanese food names file (required)\n        en_file: Path to the English food names file (required)\n        output_file: Path to the output CSV file (required)\n    \"\"\"\n    try:\n        # Read Japanese food names\n        with open(jp_file, 'r', encoding='utf-8') as f:\n            jp_foods = [line.strip() for line in f.readlines() if line.strip()]\n        \n        # Read English food names\n        with open(en_file, 'r', encoding='utf-8') as f:\n            en_foods = [line.strip() for line in f.readlines() if line.strip()]\n        \n        # Check if both files have the same number of items\n        if len(jp_foods) != len(en_foods):\n            print(f\"Warning: Number of items don't match! Japanese: {len(jp_foods)}, English: {len(en_foods)}\")\n            print(\"The CSV will contain as many rows as the shorter list.\")\n        \n        # Create CSV file\n        with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:\n            writer = csv.writer(csvfile)\n            \n            # Write header\n            writer.writerow(['jp_food', 'en_food'])\n            \n            # Write food pairs\n            for jp, en in zip(jp_foods, en_foods):\n                writer.writerow([jp, en])\n        \n        print(f\"Successfully created {output_file}\")\n        print(f\"Total rows written: {min(len(jp_foods), len(en_foods))}\")\n        \n        # Show first few rows as preview\n        print(\"\\nFirst 5 rows of the CSV:\")\n        with open(output_file, 'r', encoding='utf-8') as f:\n            reader = csv.reader(f)\n            for i, row in enumerate(reader):\n                if i < 6:  # Header + 5 data rows\n                    print(f\"{row[0]:<30} | {row[1]}\")\n                else:\n                    break\n                    \n    except FileNotFoundError as e:\n        print(f\"Error: Could not find file - {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Run the function with constants\ncreate_food_csv(JP_FILE, EN_FILE, OUTPUT_FILE)"
  },
  {
   "cell_type": "markdown",
   "id": "3b453971",
   "metadata": {},
   "source": [
    "finally, use the dictionary to translate meals to english"
   ]
  },
  {
   "cell_type": "code",
   "id": "6016f700",
   "metadata": {},
   "outputs": [],
   "source": "# translate_foods.py\nimport pandas as pd\nimport json\n\n# Constants\nINPUT_FILE = \"NHO-Kyoto-NutriBench.csv\"\nOUTPUT_FILE = \"NutriBench Multilingual Data Format.csv\"\nDICT_FILE = \"kyoto-foods-dict.csv\"\n\ndef load_translation_dictionary(dict_file):\n    \"\"\"Load the Japanese-English food dictionary into a dict\"\"\"\n    df = pd.read_csv(dict_file)\n    # Create a dictionary for easy lookup\n    translation_dict = dict(zip(df['jp_food'], df['en_food']))\n    return translation_dict\n\ndef translate_meal(meal_list, translation_dict):\n    \"\"\"Translate a list of Japanese food items to English\"\"\"\n    translated_meal = []\n    untranslated_items = []\n    \n    for item in meal_list:\n        if item in translation_dict:\n            translated_meal.append(translation_dict[item])\n        else:\n            # Item not found in dictionary\n            translated_meal.append(f\"[{item}]\")  # Mark untranslated items\n            untranslated_items.append(item)\n    \n    return translated_meal, untranslated_items\n\n# Load translation dictionary\nprint(f\"Loading translation dictionary from {DICT_FILE}...\")\ntranslation_dict = load_translation_dictionary(DICT_FILE)\nprint(f\"Loaded {len(translation_dict)} translations\")\n\n# Load meals data\nprint(f\"\\nLoading meals data from {INPUT_FILE}...\")\nmeals_df = pd.read_csv(INPUT_FILE)\nprint(f\"Loaded {len(meals_df)} meals\")\n\n# Translate meals\nprint(\"\\nTranslating meals...\")\ntranslated_meals = []\nall_untranslated = set()\n\nfor idx, row in meals_df.iterrows():\n    # Parse the JSON array from description_local\n    try:\n        meal_items = json.loads(row['description_local'])\n    except:\n        print(f\"Error parsing meal at row {idx}\")\n        translated_meals.append([])\n        continue\n    \n    # Translate the meal\n    translated_meal, untranslated = translate_meal(meal_items, translation_dict)\n    translated_meals.append(translated_meal)\n    all_untranslated.update(untranslated)\n\n# Add translated meals to dataframe\nmeals_df['description'] = [json.dumps(meal, ensure_ascii=False) for meal in translated_meals]\n\n# Save results\nprint(f\"\\nSaving translated meals to {OUTPUT_FILE}...\")\nmeals_df.to_csv(OUTPUT_FILE, index=False)\n\n# Print summary\nprint(f\"\\nTranslation complete!\")\nprint(f\"Total meals translated: {len(translated_meals)}\")\n\nif all_untranslated:\n    print(f\"\\nItems not found in dictionary ({len(all_untranslated)}):\")\n    for item in sorted(all_untranslated):\n        print(f\"  - {item}\")\n\n# Show a few examples\nprint(\"\\n--- Example Translations ---\")\nfor i in range(min(3, len(meals_df))):\n    print(f\"\\nMeal {i+1} ({meals_df.iloc[i]['meal_time']}):\")\n    original = json.loads(meals_df.iloc[i]['description_local'])\n    translated = translated_meals[i]\n    print(\"  Original:\", original)\n    print(\"  Translated:\", translated)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}